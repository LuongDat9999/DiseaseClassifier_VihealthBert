{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3Ck6eECCjVwndGo7B2Aj+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# B∆Ø·ªöC 1: C√†i ƒë·∫∑t v√† import\n","!pip install fastapi uvicorn pyvi pyngrok\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import AutoModel, AutoTokenizer\n","from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","from underthesea import word_tokenize\n","import warnings\n","warnings.filterwarnings('ignore')\n","from fastapi import FastAPI, HTTPException\n","from pydantic import BaseModel\n","from typing import List\n","import uvicorn\n","import re\n","import threading\n","import time\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","class Config:\n","    \"\"\"Configuration class for hyperparameters\"\"\"\n","    model_name = \"demdecuong/vihealthbert-base-word\"\n","    max_length = 256\n","    batch_size = 16\n","    learning_rate = 2e-5\n","    weight_decay = 0.01\n","    num_epochs = 20\n","    warmup_steps = 100\n","    dropout_rate = 0.3\n","    hidden_size = 128\n","    patience = 5\n","    gradient_clip = 1.0\n","\n","class EnhancedDiseaseClassifier(nn.Module):\n","    \"\"\"Enhanced classifier for disease prediction\"\"\"\n","    def __init__(self, pretrained_model, num_labels, config):\n","        super(EnhancedDiseaseClassifier, self).__init__()\n","        self.bert = pretrained_model\n","        self.config = config\n","\n","        # Freeze some layers for better transfer learning\n","        for param in self.bert.embeddings.parameters():\n","            param.requires_grad = False\n","\n","        # Enhanced classifier head\n","        self.dropout1 = nn.Dropout(config.dropout_rate)\n","        self.fc1 = nn.Linear(self.bert.config.hidden_size, config.hidden_size)\n","        self.dropout2 = nn.Dropout(config.dropout_rate)\n","        self.fc2 = nn.Linear(config.hidden_size, config.hidden_size // 2)\n","        self.dropout3 = nn.Dropout(config.dropout_rate)\n","        self.classifier = nn.Linear(config.hidden_size // 2, num_labels)\n","\n","        # Layer normalization\n","        self.layer_norm1 = nn.LayerNorm(config.hidden_size)\n","        self.layer_norm2 = nn.LayerNorm(config.hidden_size // 2)\n","\n","        # Initialize weights\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        \"\"\"Initialize weights properly\"\"\"\n","        for module in [self.fc1, self.fc2, self.classifier]:\n","            nn.init.xavier_normal_(module.weight)\n","            nn.init.constant_(module.bias, 0)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        # Get BERT outputs\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        # Use mean pooling instead of just CLS token\n","        last_hidden_state = outputs.last_hidden_state\n","\n","        # Mean pooling with attention mask\n","        if attention_mask is not None:\n","            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","            sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n","            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n","            pooled_output = sum_embeddings / sum_mask\n","        else:\n","            pooled_output = torch.mean(last_hidden_state, 1)\n","\n","        # Forward through enhanced classifier\n","        x = self.dropout1(pooled_output)\n","        x = F.relu(self.layer_norm1(self.fc1(x)))\n","        x = self.dropout2(x)\n","        x = F.relu(self.layer_norm2(self.fc2(x)))\n","        x = self.dropout3(x)\n","        logits = self.classifier(x)\n","\n","        return logits\n","\n","def preprocess_text(text):\n","    \"\"\"Enhanced text preprocessing for Vietnamese medical text\"\"\"\n","    if isinstance(text, str):\n","        try:\n","            # Word tokenization for Vietnamese\n","            segmented = word_tokenize(text, format=\"text\")\n","            return segmented.strip()\n","        except Exception as e:\n","            print(f\"Error in text preprocessing: {e}\")\n","            return text.strip()\n","    return \"\"\n","\n","def predict_disease(model, tokenizer, label_encoder, text, device, max_length=256, top_k=3):\n","    model.eval()\n","    processed_text = preprocess_text(text)\n","    encoding = tokenizer(\n","        processed_text,\n","        add_special_tokens=True,\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors='pt'\n","    )\n","    input_ids = encoding['input_ids'].to(device)\n","    attention_mask = encoding['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        logits = model(input_ids, attention_mask)\n","        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n","\n","    top_indices = np.argsort(probs)[::-1][:top_k]\n","    top_labels = label_encoder.inverse_transform(top_indices)\n","    top_probs = probs[top_indices]\n","    return top_labels, top_probs, probs\n","\n","def predict_disease_with_rules(text, disease_keywords, label_encoder):\n","    input_words = set(preprocess_text(text).lower().split())\n","    scores = []\n","    for label in label_encoder.classes_:\n","        keywords = set(disease_keywords.get(label, []))\n","        score = len(input_words & keywords) / (len(keywords) or 1)\n","        scores.append(score)\n","    return np.array(scores)\n","\n","def hybrid_predict(model, tokenizer, label_encoder, text, device, disease_keywords, max_length=256, top_k=3, alpha=0.7):\n","    # Model prediction\n","    top_labels, top_probs, probs = predict_disease(model, tokenizer, label_encoder, text, device, max_length, top_k=len(label_encoder.classes_))\n","    # Rule-based prediction\n","    rule_scores = predict_disease_with_rules(text, disease_keywords, label_encoder)\n","    # Weighted sum\n","    combined = alpha * probs + (1 - alpha) * rule_scores\n","    top_indices = np.argsort(combined)[::-1][:top_k]\n","    top_labels = label_encoder.inverse_transform(top_indices)\n","    top_scores = combined[top_indices]\n","    return top_labels, top_scores, combined\n","\n","# H√†m chuy·ªÉn t√™n b·ªánh sang d·∫°ng t·ª± nhi√™n\n","def beautify_label(label: str) -> str:\n","    return re.sub(r'_+', ' ', label).strip()\n","\n","# ƒê·ªãnh nghƒ©a request/response schema\n","class PredictRequest(BaseModel):\n","    text: str\n","\n","class DiseasePrediction(BaseModel):\n","    disease: str\n","    confidence: float\n","\n","class PredictResponse(BaseModel):\n","    predictions: List[DiseasePrediction]\n","\n","# Load model\n","print(\"Loading model...\")\n","\n","# Disease keywords\n","disease_keywords = {\n","    \"vi√™m_ph·ªïi\": [\"ho\", \"ƒëau\", \"ng·ª±c\", \"kh√≥\", \"th·ªü\", \"s·ªët\", \"ƒë·ªùm\", \"m√°u\"],\n","    \"c·∫£m_c√∫m\": [\"s·ªët\", \"ƒëau\", \"ƒë·∫ßu\", \"m·ªát\", \"m·ªèi\", \"ho\", \"ngh·∫πt\", \"m≈©i\", \"·ªõn\", \"l·∫°nh\"],\n","    \"vi√™m_d·∫°_d√†y\": [\"ƒëau\", \"b·ª•ng\", \"n√¥n\", \"kh√≥\", \"ti√™u\", \"·ª£\", \"h∆°i\", \"chua\", \"th∆∞·ª£ng\", \"v·ªã\"],\n","    \"ti·ªÉu_ƒë∆∞·ªùng\": [\"kh√°t\", \"n∆∞·ªõc\", \"ti·ªÉu\", \"nhi·ªÅu\", \"m·ªát\", \"m·ªèi\", \"g·∫ßy\", \"ƒë√≥i\"],\n","    \"cao_huy·∫øt_√°p\": [\"ƒëau\", \"ƒë·∫ßu\", \"ch√≥ng\", \"m·∫∑t\", \"hoa\", \"m·∫Øt\", \"g√°y\", \"tim\", \"ƒë·∫≠p\"],\n","    \"vi√™m_h·ªçng\": [\"ƒëau\", \"h·ªçng\", \"kh√≥\", \"nu·ªët\", \"s∆∞ng\", \"amidan\", \"kh√†n\", \"gi·ªçng\"],\n","    \"ƒëau_l∆∞ng\": [\"ƒëau\", \"l∆∞ng\", \"c·ª©ng\", \"t√™\", \"bu·ªët\", \"ch√¢n\", \"c·ªôt\", \"s·ªëng\"],\n","    \"ƒëau_ƒë·∫ßu\": [\"ƒëau\", \"ƒë·∫ßu\", \"nh·ª©c\", \"n·ª≠a\", \"migraine\", \"cƒÉng\", \"th·∫≥ng\", \"bu·ªìn\", \"n√¥n\"],\n","    \"vi√™m_xoang\": [\"ngh·∫πt\", \"m≈©i\", \"ƒëau\", \"ƒë·∫ßu\", \"m√°\", \"m·ªß\", \"√°p\", \"l·ª±c\"],\n","    \"r·ªëi_lo·∫°n_ti√™u_h√≥a\": [\"ti√™u\", \"ch·∫£y\", \"t√°o\", \"b√≥n\", \"ƒë·∫ßy\", \"h∆°i\", \"ngo√†i\", \"l·ªèng\"]\n","}\n","\n","# Initialize label encoder\n","label_encoder = LabelEncoder()\n","label_encoder.fit(list(disease_keywords.keys()))\n","\n","# Load model checkpoint\n","from huggingface_hub import hf_hub_download\n","\n","path_checkpoint = hf_hub_download(\n","    repo_id=\"LuongDat/heath_api\",\n","    filename=\"vihealthbert_disease_model.pth\"\n",")\n","\n","# Load config and tokenizer\n","config = Config()\n","tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","vihealthbert = AutoModel.from_pretrained(config.model_name)\n","\n","# Create model and load trained weights\n","model = EnhancedDiseaseClassifier(vihealthbert, len(label_encoder.classes_), config)\n","\n","checkpoint = torch.load(path_checkpoint, map_location=torch.device('cpu'), weights_only=False)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.eval()\n","\n","device = torch.device('cpu')\n","\n","print(\"Model loaded successfully!\")\n","\n","# Kh·ªüi t·∫°o FastAPI\n","app = FastAPI(\n","    title=\"ViHealthBERT Disease Prediction API\",\n","    description=\"API for predicting diseases from Vietnamese medical text\",\n","    version=\"1.0.0\"\n",")\n","\n","@app.get(\"/\")\n","async def root():\n","    \"\"\"Root endpoint\"\"\"\n","    return {\n","        \"message\": \"ViHealthBERT Disease Prediction API\",\n","        \"status\": \"running\",\n","        \"endpoints\": {\n","            \"predict\": \"/predict\",\n","            \"health\": \"/health\"\n","        }\n","    }\n","\n","@app.get(\"/health\")\n","async def health_check():\n","    \"\"\"Health check endpoint\"\"\"\n","    return {\n","        \"status\": \"healthy\",\n","        \"model_loaded\": model is not None,\n","        \"tokenizer_loaded\": tokenizer is not None\n","    }\n","\n","@app.post(\"/predict\", response_model=PredictResponse)\n","async def predict(request: PredictRequest):\n","    \"\"\"Predict disease from text symptoms\"\"\"\n","    try:\n","        text = request.text.strip()\n","        if not text:\n","            raise HTTPException(status_code=400, detail=\"Text cannot be empty\")\n","\n","        # Get top 2 predictions\n","        top_labels, top_scores, _ = hybrid_predict(\n","            model, tokenizer, label_encoder, text, device, disease_keywords,\n","            max_length=256, top_k=2, alpha=0.7\n","        )\n","\n","        predictions = [\n","            DiseasePrediction(disease=beautify_label(label), confidence=float(score))\n","            for label, score in zip(top_labels, top_scores)\n","        ]\n","\n","        return PredictResponse(predictions=predictions)\n","\n","    except Exception as e:\n","        print(f\"Prediction error: {e}\")\n","        raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n","\n","# Ch·∫°y server trong thread ri√™ng\n","def run_server():\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n","\n","# Kh·ªüi t·∫°o v√† start thread\n","server_thread = threading.Thread(target=run_server, daemon=True)\n","server_thread.start()\n","\n","# ƒê·ª£i server kh·ªüi ƒë·ªông\n","time.sleep(3)\n","\n","print(\"üöÄ API is running at: http://localhost:8000\")\n","print(\"üìö Documentation at: http://localhost:8000/docs\")\n","print(\"üè• Health check at: http://localhost:8000/health\")\n","print(\"‚úÖ Server is running in background. You can now run other cells!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPxo3B6-bTBh","executionInfo":{"status":"ok","timestamp":1753263665404,"user_tz":-420,"elapsed":47172,"user":{"displayName":"L∆∞∆°ng ƒê·∫°t","userId":"11214560588962627512"}},"outputId":"00c7c6b6-52ca-46bc-ec3b-f2ae2f8f7e87"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n","Requirement already satisfied: underthesea in /usr/local/lib/python3.11/dist-packages (6.8.4)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n","Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n","Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.11/dist-packages (from underthesea) (0.9.11)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.5.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.6.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\n","Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.0.4)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n","Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->underthesea) (2024.11.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.7.14)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.16.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (3.6.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n","Loading model...\n","Model loaded successfully!\n"]},{"output_type":"stream","name":"stderr","text":["INFO:     Started server process [214]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use\n","INFO:     Waiting for application shutdown.\n","INFO:     Application shutdown complete.\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ API is running at: http://localhost:8000\n","üìö Documentation at: http://localhost:8000/docs\n","üè• Health check at: http://localhost:8000/health\n","‚úÖ Server is running in background. You can now run other cells!\n"]}]},{"cell_type":"code","source":["!pip freeze > requirements.txt"],"metadata":{"id":"mFBBhyBYyo5T","executionInfo":{"status":"ok","timestamp":1753265914546,"user_tz":-420,"elapsed":1521,"user":{"displayName":"L∆∞∆°ng ƒê·∫°t","userId":"11214560588962627512"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# TEST 1: D√πng requests (c·∫ßn ƒë·ª£i 5 gi√¢y ƒë·ªÉ server kh·ªüi ƒë·ªông)\n","import requests\n","import json\n","import time\n","\n","time.sleep(2)  # ƒê·ª£i server s·∫µn s√†ng\n","\n","try:\n","    # Test health check\n","    health_response = requests.get(\"http://localhost:8000/health\")\n","    print(\"Health check:\", health_response.json())\n","\n","    # Test prediction\n","    url = \"http://localhost:8000/predict\"\n","    data = {\"text\": \"T√¥i b·ªã ho, s·ªët v√† kh√≥ th·ªü\"}\n","\n","    response = requests.post(url, json=data)\n","    print(\"Prediction result:\", response.json())\n","\n","except Exception as e:\n","    print(f\"Error: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwE5-sYncstT","executionInfo":{"status":"ok","timestamp":1753263685842,"user_tz":-420,"elapsed":2780,"user":{"displayName":"L∆∞∆°ng ƒê·∫°t","userId":"11214560588962627512"}},"outputId":"78a931db-d088-4992-e5cf-4b010b9375a1"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:     127.0.0.1:46102 - \"GET /health HTTP/1.1\" 200 OK\n","Health check: {'status': 'healthy', 'model_loaded': True, 'tokenizer_loaded': True}\n","INFO:     127.0.0.1:46114 - \"POST /predict HTTP/1.1\" 200 OK\n","Prediction result: {'predictions': [{'disease': 'vi√™m ph·ªïi', 'confidence': 0.6268409729003906}, {'disease': 'vi√™m h·ªçng', 'confidence': 0.18758623898029328}]}\n"]}]}]}